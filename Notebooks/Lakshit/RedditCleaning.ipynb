{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46917c-d83a-466c-8d9f-4ad00c1619b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6c5be-b828-4a27-b7ac-852e899c8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_alcohol_posts_and_comments.csv\",  \n",
    "    \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_posts_and_comments_1000.csv\",   \n",
    "    \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_wholesale_posts_and_comments_1000.csv\"    \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f15ffd-6a1c-455f-9870-dc75963bce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.dropna(subset=[\"post_title\", \"comment_body\"])  # Remove rows with missing text\n",
    "    df[\"comment_author\"] = df[\"comment_author\"].fillna(\"Anonymous\")  \n",
    "\n",
    "    # Convert UNIX timestamps to readable datetime\n",
    "    df[\"post_created\"] = pd.to_datetime(df[\"post_created\"], unit=\"s\")\n",
    "    df[\"comment_created\"] = pd.to_datetime(df[\"comment_created\"], unit=\"s\")\n",
    "\n",
    "    # Standardize text: convert to lowercase and strip extra whitespace\n",
    "    df[\"post_title\"] = df[\"post_title\"].str.lower().str.strip()\n",
    "    df[\"comment_body\"] = df[\"comment_body\"].str.lower().str.strip()\n",
    "\n",
    "    # Remove special characters (optional)\n",
    "    df[\"post_title\"] = df[\"post_title\"].str.replace(r\"[^a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "    df[\"comment_body\"] = df[\"comment_body\"].str.replace(r\"[^a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "\n",
    "    # Filter out deleted comments\n",
    "    df = df[~df[\"comment_body\"].isin([\"[deleted]\", \"[removed]\"])]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd9e67-5bf5-4a9e-bd60-a0416c9a67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files\n",
    "cleaned_dataframes = [clean_data(file) for file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621f3a7-2c50-41a7-863a-ae2895677c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all cleaned datasets\n",
    "final_df = pd.concat(cleaned_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5c69f-bfef-4c26-893b-797343a93a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned dataset\n",
    "final_cleaned_file = \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/Cleaned Dataset/cleaned_combined_reddit_costco_data.csv\"\n",
    "final_df.to_csv(final_cleaned_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3ee72-fc7c-47f0-85fd-fafc86930102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data cleaning completed. Combined cleaned data saved to {final_cleaned_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6188b-0535-437d-8be3-277b221f3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pip install spacy\n",
    "#!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa90ae-f7c8-4781-90a3-7cf6eaa9b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15505199-580e-4057-b863-72c3aa322ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define keywords for categorization\n",
    "\n",
    "product_keywords = [\n",
    "    \"kirkland\", \"kirkland signature\", \"kirkland wine\", \"kirkland coffee\", \"kirkland batteries\",\n",
    "    \"kirkland dog food\", \"kirkland vodka\", \"kirkland whiskey\",\n",
    "    \n",
    "    \"laptop\", \"tv\", \"computer\", \"printer\", \"headphones\", \"smartphone\", \"tablet\", \"camera\",\n",
    "    \"sound system\", \"gaming console\", \"airpods\", \"apple\", \"samsung\", \"hp\", \"dell\", \"sony\",\n",
    "\n",
    "    \"pizza\", \"wine\", \"groceries\", \"milk\", \"eggs\", \"bread\", \"cheese\", \"meat\", \"snacks\",\n",
    "    \"beverages\", \"chicken\", \"sushi\", \"frozen food\", \"organic food\", \"costco bakery\",\n",
    "    \"dairy\", \"costco food court\", \"hot dog\", \"beef\", \"seafood\", \"kirkland nuts\",\n",
    "\n",
    "   \n",
    "    \"furniture\", \"sofa\", \"table\", \"mattress\", \"bed\", \"chair\", \"desk\", \"office furniture\",\n",
    "    \"home decor\", \"costco warehouse furniture\",\n",
    "\n",
    "   \n",
    "    \"clothing\", \"shoes\", \"jackets\", \"coats\", \"jeans\", \"t-shirts\", \"athletic wear\",\n",
    "    \"underwear\", \"socks\",\n",
    "\n",
    "  \n",
    "    \"vitamins\", \"supplements\", \"protein powder\", \"costco pharmacy\", \"medicine\",\n",
    "    \"eyeglasses\", \"contact lenses\", \"hearing aids\", \"first aid kits\",\n",
    "\n",
    " \n",
    "    \"detergent\", \"toilet paper\", \"paper towels\", \"dishwasher pods\", \"cleaning supplies\",\n",
    "    \"costco kirkland soap\", \"garbage bags\", \"air fresheners\", \"dish soap\",\n",
    "\n",
    " \n",
    "    \"tires\", \"camping\", \"bbq\", \"grill\", \"patio furniture\", \"generators\", \"outdoor lighting\",\n",
    "    \"batteries\", \"car accessories\", \"lawn mower\"\n",
    "]\n",
    "\n",
    "service_keywords = [\n",
    "    \"membership\", \"costco executive membership\", \"renewal\", \"membership upgrade\",\n",
    "    \"costco gold star\", \"return policy\", \"refund\", \"warranty\", \"extended warranty\",\n",
    "    \"costco gas\", \"costco receipt lookup\",\n",
    "\n",
    "    \"customer service\", \"help desk\", \"assistance\", \"complaint\", \"staff\", \"employees\",\n",
    "    \"rude service\", \"checkout\", \"lines\", \"crowded\", \"experience\", \"store hours\",\n",
    "    \n",
    "    \"delivery\", \"shipping\", \"same-day delivery\", \"instacart\", \"costco.com\", \"costco app\",\n",
    "    \"online order\", \"tracking\", \"shipping delay\", \"pickup service\", \"curbside pickup\",\n",
    "    \"in-store pickup\",\n",
    "\n",
    "    \"discounts\", \"offers\", \"promo\", \"coupons\", \"deals\", \"costco sales\", \"cashback\",\n",
    "    \"executive rewards\", \"gift cards\", \"rebates\", \"costco credit card\", \"membership perks\",\n",
    "\n",
    "    \"costco travel\", \"vacation\", \"rental cars\", \"travel discounts\", \"car insurance\",\n",
    "    \"health insurance\", \"costco visa card\", \"capital one\", \"financing\", \"costco mortgage services\",\n",
    "    \"travel\"\n",
    "\n",
    "    \"gas\", \"fuel\", \"car wash\", \"costco gas station\", \"tire center\", \"battery replacement\",\n",
    "    \"oil change\", \"auto repair\", \"tire\"\n",
    "]\n",
    "\n",
    "#product_keywords = [\"kirkland\", \"laptop\", \"pizza\", \"wine\", \"electronics\", \"furniture\", \"tvs\", \"groceries\", \"clothing\"]\n",
    "#service_keywords = [\"membership\", \"customer service\", \"refund\", \"return policy\", \"delivery\", \"checkout\", \"discounts\"]\n",
    "\n",
    "# Path to the combined cleaned dataset\n",
    "cleaned_file_path = \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/Cleaned Dataset/cleaned_combined_reddit_costco_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265d8b6-90f7-4176-a58f-b1a838355167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(text, keywords):\n",
    "    \"\"\"\n",
    "    Computes similarity score between a text and category keywords using SpaCy word embeddings.\n",
    "    Handles empty vectors to avoid warnings.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return 0  \n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    similarities = []\n",
    "    for word in keywords:\n",
    "        word_doc = nlp(word)\n",
    "        if doc.has_vector and word_doc.has_vector: \n",
    "            similarities.append(doc.similarity(word_doc))\n",
    "\n",
    "    return max(similarities) if similarities else 0  \n",
    "\n",
    "\n",
    "def categorize_post(post_title, comment_body):\n",
    "    \"\"\"\n",
    "    Categorizes the post as 'Product', 'Service', or 'General Discussion' based on keywords & semantic similarity.\n",
    "    \"\"\"\n",
    "    text = f\"{post_title} {comment_body}\".lower()\n",
    "\n",
    "    if any(re.search(rf\"\\b{kw}\\b\", text) for kw in product_keywords):\n",
    "        return \"Product\"\n",
    "    if any(re.search(rf\"\\b{kw}\\b\", text) for kw in service_keywords):\n",
    "        return \"Service\"\n",
    "\n",
    "    # Step 2: Semantic Similarity (Threshold: 0.7)\n",
    "    product_similarity = semantic_similarity(text, product_keywords)\n",
    "    service_similarity = semantic_similarity(text, service_keywords)\n",
    "\n",
    "    if product_similarity > 0.7:\n",
    "        return \"Product\"\n",
    "    elif service_similarity > 0.7:\n",
    "        return \"Service\"\n",
    "\n",
    "    return \"General Discussion\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4c42c-5b12-472e-9eb8-285d864d5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "df[\"post_title\"] = df[\"post_title\"].fillna(\"\")\n",
    "df[\"comment_body\"] = df[\"comment_body\"].fillna(\"\")\n",
    "\n",
    "df[\"category\"] = df.apply(lambda row: categorize_post(row[\"post_title\"], row[\"comment_body\"]), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba51c1-3d73-45d7-a484-82b87539a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save categorized dataset\n",
    "categorized_file_path = \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/Cleaned Dataset/categorized_reddit_costco_data.csv\"\n",
    "df.to_csv(categorized_file_path, index=False)\n",
    "\n",
    "print(f\"Data categorization completed. Categorized data saved to {categorized_file_path}\")\n",
    "print(df[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd56087-412b-4a85-a5c2-8de5105cf5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f0833-1936-4930-8c43-f42c8e5b23dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f91d40-c441-4433-b0c3-b2ca3b1ccefb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dabc7c-ad19-4600-8f1e-c2774c9912df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e587f-4c38-4dcf-b8b0-c7ffc0fbe545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a5e95-a9a6-4d17-8ace-94629fb1414d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654f6bb6-8a7d-4615-b204-480698825c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "  Downloading cmake-3.31.4-py3-none-macosx_10_10_universal2.whl.metadata (6.5 kB)\n",
      "Downloading cmake-3.31.4-py3-none-macosx_10_10_universal2.whl (47.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cmake\n",
      "Successfully installed cmake-3.31.4\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pyarrow==12.0.0 (from versions: 14.0.0, 14.0.1, 14.0.2, 15.0.0, 15.0.1, 15.0.2, 16.0.0, 16.1.0, 17.0.0, 18.0.0, 18.1.0, 19.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pyarrow==12.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0mFound existing installation: datasets 3.2.0\n",
      "Uninstalling datasets-3.2.0:\n",
      "  Successfully uninstalled datasets-3.2.0\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Installing collected packages: datasets\n",
      "Successfully installed datasets-3.2.0\n",
      "Name: pyarrow\n",
      "Version: 19.0.0\n",
      "Summary: Python library for Apache Arrow\n",
      "Home-page: https://arrow.apache.org/\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache Software License\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: dask-expr, datasets, streamlit, triad\n",
      "Name: datasets\n",
      "Version: 3.2.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "#pip install transformers\n",
    "#!pip install datasets\n",
    "\n",
    "# 1. Install cmake (needed for building pyarrow)\n",
    "!pip install cmake\n",
    "\n",
    "# 2. Downgrade pyarrow to a stable version (e.g., 12.0.0 or 6.0.1)\n",
    "!pip install pyarrow==12.0.0 --no-build-isolation --only-binary=:all:\n",
    "\n",
    "# 3. Uninstall the current version of datasets\n",
    "!pip uninstall datasets -y\n",
    "\n",
    "# 4. Reinstall the latest compatible version of datasets\n",
    "!pip install datasets --upgrade\n",
    "\n",
    "# 5. Verify installation of pyarrow and datasets\n",
    "!pip show pyarrow\n",
    "!pip show datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68bb5ff-e802-4e6a-bd62-ac6b11aa5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f81464-5624-4732-8707-0de4cb7ccddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Map categories to numeric labels for BERT\u001b[39;00m\n\u001b[1;32m     15\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneral Discussion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(label_mapping)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and validation sets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m train_texts, val_texts, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     20\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'category'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_file_path = \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/Cleaned Dataset/cleaned_combined_reddit_costco_data.csv\"\n",
    "df = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Combine post title and comment body into a single text column\n",
    "df[\"post_title\"] = df[\"post_title\"].fillna(\"\")\n",
    "df[\"comment_body\"] = df[\"comment_body\"].fillna(\"\")\n",
    "df[\"text\"] = df[\"post_title\"] + \" \" + df[\"comment_body\"]\n",
    "\n",
    "# Map categories to numeric labels for BERT\n",
    "label_mapping = {\"Product\": 0, \"Service\": 1, \"General Discussion\": 2}\n",
    "df[\"label\"] = df[\"category\"].map(label_mapping)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a504253-59d4-4ded-bae2-2ca518fcaedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and tokenize the dataset\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n",
    "# Create a PyTorch Dataset class for BERT\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = RedditDataset(train_encodings, train_labels)\n",
    "val_dataset = RedditDataset(val_encodings, val_labels)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define a compute metrics function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677203a5-0eda-4e98-94b7-e0da0b131a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")\n",
    "print(\"Model fine-tuning completed and saved.\")\n",
    "\n",
    "# ================================\n",
    "# Apply the Model to the Entire Dataset\n",
    "# ================================\n",
    "# Load the saved model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a452f3-1db4-4222-975f-073bcb84a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_bert\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_bert\")\n",
    "\n",
    "def classify_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    reverse_mapping = {0: \"Product\", 1: \"Service\", 2: \"General Discussion\"}\n",
    "    return reverse_mapping[predicted_class]\n",
    "\n",
    "# Classify the entire dataset\n",
    "df[\"predicted_category\"] = df[\"text\"].apply(classify_text)\n",
    "\n",
    "# Save the classified dataset\n",
    "classified_file_path = \"/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/Cleaned Dataset/classified_reddit_costco_data.csv\"\n",
    "df.to_csv(classified_file_path, index=False)\n",
    "\n",
    "print(f\"Classification completed. Classified data saved to {classified_file_path}\")\n",
    "print(df[\"predicted_category\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
